import torch
import torch.nn as nn
import torch.nn.functional as F

### MLP with lienar output
class MLP(nn.Module):
    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):
        '''
            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.
            input_dim: dimensionality of input features
            hidden_dim: dimensionality of hidden units at ALL layers
            output_dim: number of classes for prediction
            device: which device to use

            eu am inteles ca se creează rețeaua cu un input layer, hidden layers și putput layer
            după fiecare hidden layer avem un alt layer special (batch) în care se activează hidden layerul 
        '''
    
        super(MLP, self).__init__()

        self.linear_or_not = True #default is linear model
        self.num_layers = num_layers

        if num_layers < 1:
            raise ValueError("number of layers should be positive!")
        elif num_layers == 1:
            #Linear model
            self.linear = nn.Linear(input_dim, output_dim)
            #input_dim=specifies the size of the input feature space.
            #output_dim=Each input sample will be transformed into a vector of size output_dim after passing through this linear layer.
        else:
            #Multi-layer model
            self.linear_or_not = False
            #variabila linears va reprezenta o lista de layere liniare, spre deosebire de variabila linear ce era UN SINGUR layer liniar
            self.linears = torch.nn.ModuleList()
            self.batch_norms = torch.nn.ModuleList() #la fel ca la linears, bathc_norms reprezinta o lista de ceva, vedem...

            #se creează structura GNN, un input layer, hidden layers și output layer
            self.linears.append(nn.Linear(input_dim, hidden_dim))
            for layer in range(num_layers - 2):
                self.linears.append(nn.Linear(hidden_dim, hidden_dim))
            self.linears.append(nn.Linear(hidden_dim, output_dim))

            #Overall, this loop iterates over the hidden layers of the MLP and appends batch normalization layers
            #These batch normalization layers will be used to normalize the activations of the hidden layers during training
            for layer in range(num_layers - 1):
                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))

    def forward(self, x):
        if self.linear_or_not:
            #If linear model
            return self.linear(x)
        else:
            #If MLP
            h = x # initial, reprezentarea grafului (h) este chiar feature map-ul (x)
            #presupun că se scrie funcția de activare pentru hidden layers
            #this forward pass iterates over the hidden layers, applying linear transformations, batch normalization, and ReLU activation function
            for layer in range(self.num_layers - 1):
                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))
            return self.linears[self.num_layers - 1](h)
